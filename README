This is a rudimentary start at a workable elastic ceph charm.

To deploy, simply deploy it as a service, and add units. All nodes will
run all components of CEPH currently (mds, osd, and mon).  This means
you should not try to use the cluster until it has reached an odd number
of machines.

Because I haven't worked out how to do mkcephfs properly:

First deploy on one node. SSH to it, and run:

sudo mkcephfs -a -c /etc/ceph/ceph.conf

It should create a ceph filesystem with data stored on /mnt.  On EC2
instances, this is automatically a large ephemeral drive with ext4,
and should perform reasonably well.

After this, add-unit to grow/shrink the cluster. Use the 'run-xxx'
flags and remote-(mds|osd|mon) to relate one service with another.

When you are not adding units, its probably best to disable the root
ssh with:

juju set name-of-service root-ssh=no

Once done, one should be able to mount the ceph filesystem using any of
the service unit IP's.

After you are done, you can improve security by turning off the root ssh,
which is only used for mkcephfs, with:

